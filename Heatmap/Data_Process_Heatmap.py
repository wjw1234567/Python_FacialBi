import pandas as pd
from collections import deque
from clickhouse_driver import Client
from datetime import datetime



# ==========================
# 1. ClickHouse è¿æ¥
# ==========================


# è¿æ¥ ClickHouse
client = Client(
    host='localhost',
    port=9000,
    user='default',
    password='ck_test',
    database='Facial',
    send_receive_timeout=60   # é¿å…å†™å…¥è¶…æ—¶
)


# ==========================
# 2. æ»‘çª—å¤„ç†å‡½æ•° (deque ä¼˜åŒ–ç‰ˆ)
# ==========================
def process_partition(date, person_id, client):
    """
    é’ˆå¯¹å•ä¸ª (date, person_id) åˆ†åŒºè¿›è¡Œæ»‘çª—å¤„ç†ï¼ˆdequeä¼˜åŒ–ç‰ˆï¼‰
    """
    query = f"""
    
    SELECT toDate(capture_time) date
       ,profile_id
       ,region_name
       ,capture_time
FROM  dwd_user_capture_original_example
where date='{date}'  and profile_id= '{person_id}'
order by capture_time;
    
    
    
    """


    df,columns_with_types = client.execute(query, with_column_types=True)
    columns = [col[0] for col in columns_with_types]
    df = pd.DataFrame(df, columns=columns)

    # print(df)

    if df.empty:
        return None

    df["capture_time"] = pd.to_datetime(df["capture_time"])
    records = df.to_dict("records")

    target_bins = [-60, -45, -30, -15, -10, -5, 0, 5, 10, 15, 30, 45, 60]
    results = []

    window = deque()   # å­˜æ”¾ (idx, record)
    n = len(records)

    right = 0
    for i, v1 in enumerate(records):
        t1 = v1["capture_time"]
        base_area = v1["region_name"]

        # print(f"v1={v1}")
        '''
            éå†æ¯ä¸ªç‚¹ (i, v1)
            ä»¥å½“å‰ç‚¹ t1 ä¸ºâ€œä¸­å¿ƒâ€ï¼Œç»´æŠ¤ä¸€ä¸ªæ—¶é—´çª—å£ã€‚

            å³æŒ‡é’ˆæ‰©å¼ çª—å£
            æŠŠ capture_time åœ¨ [t1, t1+60åˆ†é’Ÿ] å†…çš„ç‚¹åŠ å…¥ dequeã€‚
            ğŸ‘‰ ä¿è¯å³è¾¹ä¸ä¼šæ¼æ‰æœªæ¥ 1 å°æ—¶å†…çš„ç‚¹ã€‚

            å·¦æŒ‡é’ˆæ”¶ç¼©çª—å£
            æŠŠ < t1-60åˆ†é’Ÿ çš„ç‚¹ç§»å‡ºçª—å£ã€‚
            ğŸ‘‰ ä¿è¯çª—å£å†…çš„ç‚¹å§‹ç»ˆåœ¨ [t1-60åˆ†é’Ÿ, t1+60åˆ†é’Ÿ]ã€‚

            çª—å£ç»“æœ
            æ­¤æ—¶ window ä¸­çš„æ‰€æœ‰ç‚¹ï¼Œéƒ½æ˜¯ä¸å½“å‰ç‚¹ t1 æ—¶é—´å·®ä¸è¶…è¿‡ 60 åˆ†é’Ÿçš„è®°å½•ã€‚
            ä½ å¯ä»¥åœ¨è¿™é‡Œå¯¹çª—å£é‡Œçš„ç‚¹åšèšåˆã€è®¡æ•°ã€åˆ†ç»„ç­‰æ“ä½œã€‚
        '''

        # --- 1) æ»‘åŠ¨å³æŒ‡é’ˆï¼ŒæŠŠ <= t1+60 çš„ç‚¹åŠ å…¥ deque
        while right < n and records[right]["capture_time"] <= t1 + pd.Timedelta(minutes=60):
            window.append((right, records[right]))
            right += 1

        # --- 2) æ»‘åŠ¨å·¦æŒ‡é’ˆï¼ŒæŠŠ < t1-60 çš„ç‚¹è¸¢å‡º deque
        while window and window[0][1]["capture_time"] < t1 - pd.Timedelta(minutes=60):
            window.popleft()

        # print("window=",window)

        # --- 3) éå†çª—å£å€™é€‰ï¼Œæ›´æ–°æ¯ä¸ª off_bin æœ€è¿‘ç‚¹ (argMin)
        bucket_best = {}

        for idx, v2 in window:
            # if idx == i:  # ä¸è·³è¿‡è‡ªå·±
            #     continue
            diff_min = (v2["capture_time"] - t1).total_seconds() / 60
            # off_bin = int(diff_min // 15) * 15

            if diff_min >= 0 and diff_min <= 15 and v2["capture_time"] != t1:
                off_bin = (int(diff_min // 5) + 1) * 5
            elif diff_min == 0 and v2["capture_time"] == t1:
                off_bin =0
            elif diff_min >= -15 and diff_min < 0:
                off_bin = (int(diff_min // 5)) * 5
            elif diff_min >= 16 and diff_min <= 60:
                off_bin = (int(diff_min // 15) + 1) * 15
            elif diff_min >= -60 and diff_min <= -16:
                off_bin = (int(diff_min // 15)) * 15


            if off_bin not in target_bins:
                continue
            if (off_bin not in bucket_best or abs(diff_min) < abs(bucket_best[off_bin]["diff_min"])):
                bucket_best[off_bin] = {"region_name": v2["region_name"], "diff_min": diff_min}
        # print(t1, base_area, bucket_best)




        # --- 4) ç”Ÿæˆè¾“å‡ºè¡Œ
        for off_bin,value in bucket_best.items():
            row = {
                "date": date,
                "profile_id": person_id,
                "region_name": base_area,
                "z0_time": t1,
                "off_bin":off_bin,
                "area_at_off":value["region_name"],
                "batch_time":datetime.now()
            }
            results.append(row)




        # --- 4) ç”Ÿæˆè¾“å‡ºè¡Œ
        # row = {
        #     "date": date,
        #     "profile_id": person_id,
        #     "region_name": base_area,
        #     "z0_time": t1
        # }
        # for off in target_bins:
        #     if off == 0:
        #         row["z0"] = base_area
        #     else:
        #         row[f"z{off:+d}"] = bucket_best.get(off, {}).get("region_name")
        # results.append(row)

    if not results:
        return None
    return pd.DataFrame(results)



def insert_df(client, table: str, df: pd.DataFrame):
    """
    å°† pandas.DataFrame æ‰¹é‡å†™å…¥ ClickHouse
    """
    if df.empty:
        return

    # DataFrame åˆ—åï¼ˆå¿…é¡»å’Œ ClickHouse è¡¨ç»“æ„ä¸€è‡´ï¼‰
    columns = list(df.columns)

    escaped_columns = [f"`{col}`" for col in columns]

    # è½¬æ¢æˆ list[tuple]
    data = [tuple(row) for row in df.itertuples(index=False, name=None)]

    insert_sql = f"INSERT INTO {table} ({','.join(escaped_columns)}) VALUES"
    client.execute(insert_sql, data)  # ç¬¬äºŒä¸ªå‚æ•°ç›´æ¥ä¼ å…¥æ•°æ®åˆ—è¡¨


    # è°ƒç”¨ clickhouse-connect çš„ insert
    # client.insert(table, data, column_names=columns)




# ==========================
# 3. ä¸»æµç¨‹ï¼šåˆ†æ‰¹è¯»å– â†’ å¤„ç† â†’ å†™å›
# ==========================

# where date=toDate('2025-08-20') and profile_id='p103'

def main():
    # å…ˆæ‹¿åˆ°æ‰€æœ‰åˆ†åŒºé”®

    query = f"""
             SELECT DISTINCT toDate(capture_time) date, profile_id
             FROM dwd_user_capture_original_example
             
             ORDER BY date, profile_id
         """

    # partitions = client.execute(query, with_column_types=True)

    partitions, columns_with_types = client.execute(query, with_column_types=True)
    columns = [col[0] for col in columns_with_types]

    partitions = pd.DataFrame(partitions, columns=columns)

    batch_size = 5000  # æ¯æ‰¹å†™ 500 è¡Œ
    buffer = []

    for _, row in partitions.iterrows():
        date = row["date"]
        person_id = row["profile_id"]

        df_out = process_partition(date, person_id, client)
        if df_out is None or df_out.empty:
            continue

        buffer.append(df_out)

        # --- åˆ†æ‰¹å†™å› ClickHouse
        if sum(len(x) for x in buffer) >= batch_size:
            big_df = pd.concat(buffer, ignore_index=True)
            insert_df(client,"dws_visitor_path_track_heatmap", big_df)
            print(f"å†™å…¥ {len(big_df)} è¡Œ")
            buffer.clear()

    # --- å¤„ç†å‰©ä½™æ•°æ®
    if buffer:
        big_df = pd.concat(buffer, ignore_index=True)
        insert_df(client,"dws_visitor_path_track_heatmap", big_df)
        print(f"å†™å…¥ {len(big_df)} è¡Œ (æœ€åä¸€æ‰¹)")






if __name__ == "__main__":
    main()
